#summerized core set sampling scripts derived from previous R scripts and data generated in "SP_core_set_phenotype_scripts"

#loading required packages
library(readxl)
library(stringr)
library(plyr)
library(cluster)
library(ggplot2)
library(MCDA)

###Loading previously formatted datasets####
load("SP_core_selection_data_7-30-21.Rdata")


####functions####

# a function to sample a core set from a set of clusters by first fixing chosen lines, then individually suseting and randomly sampling from all other clusters.
fix.sub.sample<-function(target.df, fixed.names=NA, fixed.PIs=NA , clust.colname="Cluster_384"){
  if(!is.na(fixed.names)){#start by fixing any specifically chosen individulas if present
    fix.ind<-target.df$`Common Name` %in% fixed.names
    df.sample<-target.df[fix.ind,]
    ind.clust.colname<-which(colnames(target.df) == clust.colname)
    fixed.clust<-levels(as.factor(df.sample[,ind.clust.colname]))
  }else if(!is.na(fixed.PIs)){
    fix.ind<-target.df$`GRIN Database PI Accession` %in% fixed.PIs
    df.sample<-target.df[fix.ind,]
    ind.clust.colname<-which(colnames(target.df) == clust.colname)
    fixed.clust<-levels(as.factor(df.sample[,ind.clust.colname]))
  }else{#if no fixed individuals
    sample<-data.frame(matrix(ncol = ncol(target.df), nrow = 0))
    colnames(sample)<-colnames(target.df)
  }
  ind.clust.colname<-which(colnames(target.df) == clust.colname)
  for(i in levels(as.factor(target.df[,ind.clust.colname]))){
    if(!is.na(fixed.names) & i %in% fixed.clust){ # if we have fixed individuals don't sample again from those clusters
      next()
    }else if(!is.na(fixed.PIs) & i %in% fixed.clust){
      next()
    }
    S<-subset(target.df, get(clust.colname) == i) #subset the cluster
    mysample <- S[sample(1:nrow(S), 1 , replace=FALSE),] #randomly sample 1 individual from the cluster
    df.sample<-rbind.data.frame(df.sample, mysample)
  }
  df.sample
}

#a function to generate a standardized metadata data.frame for each sampled core set
get.meta.data<-function(df, class.vec=class.data){
  trait<-character()
  d.quant.mean<-numeric()
  d.quant.sd<-numeric()
  d.quant.max<-numeric()
  d.quant.min<-numeric()
  d.cat.levels<-numeric()
  for(i in c(8:ncol(df))){
    #cat("working on:", i, "\n")
    if(is.na(class.vec[i])){next}
    if(sum(is.na(df[,i]))==length(df[,i])){
      trait[i]<-colnames(df)[i]
      d.quant.mean[i]<- NA
      d.quant.sd[i]<- NA
      d.quant.max[i]<- NA
      d.quant.min[i]<- NA
      d.cat.levels[i]<- NA
    }else if(class.vec[i] == "c"){#if it's a catagoricly measured trait
      trait[i]<-colnames(df)[i]
      d.quant.mean[i]<- NA
      d.quant.sd[i]<- NA
      d.quant.max[i]<- NA
      d.quant.min[i]<- NA
      d.cat.levels[i]<-length(levels(as.factor(df[,i])))
    }else if(class.vec[i] == "q"){#if it's a quantitatively measured trait
      trait[i]<-colnames(df)[i]
      d.quant.mean[i]<- mean(df[,i], na.rm = TRUE)
      d.quant.sd[i]<- sd(df[,i], na.rm = TRUE)
      d.quant.max[i]<- max(df[,i], na.rm = TRUE)
      d.quant.min[i]<- min(df[,i], na.rm = TRUE)
      d.cat.levels[i]<- NA
    }
  }
  d.meta.data<-data.frame(trait=trait, quant.mean=d.quant.mean, quant.sd=d.quant.sd, quant.max=d.quant.max, quant.min=d.quant.min, catagorical.levels=d.cat.levels)
  d.meta.data
}


#a function to generate a comparison vector between two meta dataframes generated by the function 'get.meta.data' above
comp.meta.data<-function(parent.meta=d.meta.data, sampled.meta){
  quant.tot.mean.deviation<-sum(abs(parent.meta$quant.mean - sampled.meta$quant.mean), na.rm = TRUE)#for quantifying the deviation from the original mean for each quantitative trait
  quant.avg.mean.deviation<-mean(abs(parent.meta$quant.mean - sampled.meta$quant.mean), na.rm = TRUE)
  quant.tot.sd.deviation<-sum(abs(parent.meta$quant.sd - sampled.meta$quant.sd), na.rm = TRUE)#for quantifying the deviation from the original standard deviation for each quantitative trait
  quant.avg.sd.deviation<-mean(abs(parent.meta$quant.sd - sampled.meta$quant.sd), na.rm = TRUE)
  parent.spread<-parent.meta$quant.max - parent.meta$quant.min #for quantifying the change in quantitative trait spread
  sample.spread<-sampled.meta$quant.max - sampled.meta$quant.min
  quant.spread.reduction<-(parent.spread - sample.spread) / parent.spread
  quant.avg.spread.reduction<-mean(quant.spread.reduction, na.rm = TRUE)
  quant.med.spread.reduction<-median(quant.spread.reduction, na.rm = TRUE)
  quant.max.spread.reduction<-max(quant.spread.reduction, na.rm = TRUE)
  cat.tot.phene.loss<-sum(parent.meta$catagorical.levels - sampled.meta$catagorical.levels, na.rm = TRUE)
  cat.avg.phene.loss<-mean(parent.meta$catagorical.levels - sampled.meta$catagorical.levels, na.rm = TRUE)
  cat.med.phene.loss<-median(parent.meta$catagorical.levels - sampled.meta$catagorical.levels, na.rm = TRUE)
  cat.max.phene.loss<-max(parent.meta$catagorical.levels - sampled.meta$catagorical.levels, na.rm = TRUE)
  meta.comp.vec<-c(quant.tot.mean.deviation, quant.avg.mean.deviation, quant.tot.sd.deviation, quant.avg.sd.deviation, quant.avg.spread.reduction, quant.med.spread.reduction, quant.max.spread.reduction, cat.tot.phene.loss, cat.avg.phene.loss, cat.med.phene.loss, cat.max.phene.loss)
}

#### selecting a 24 line core set #####

#first need to grab meta data from the complete set of lines to copare to
d.meta.data<-get.meta.data(Data)
#we want to decide on a 24 line core set that we will then fix those line in the selection of the larger core sets
#running permutations of selecting one line from each of the 24 clusters using the 482 lines that were manually selected based on extreme phenotypes
#run through the first sample to make sure it's doing what we want
t.sample<-fix.sub.sample(target.df = sampling.data, fixed.names = c("Beauregard", "Tanzania", "Ruddy", "Porto Rico"), clust.colname="Cluster_24") #the sampling, Note: this will throw a warning message when more than one fixed individual is designated "the condition has length > 1 and only the first element will be used" don't worry we only have to use the first element in that if statement, and all individuals designated will be fixed in the sample
t.meta.data<-get.meta.data(t.sample) # extracting meta data for this particular sampled core set
targ.24.samples<-data.frame(matrix(NA, nrow = 1, ncol = 24)) # generating the matrix to fill with selected core sets
targ.24.samples[1,]<-t.sample$Code #save the sampled line codes
row.names(targ.24.samples)[1]<-"sample_1" #name the sampled data
targ.24.meta.comps<-data.frame(quant.tot.mean.deviation=numeric(), quant.avg.mean.deviation=numeric(), quant.tot.sd.deviation=numeric(), quant.avg.sd.deviation=numeric(), quant.avg.spread.reduction=numeric(), quant.med.spread.reduction=numeric(), quant.max.spread.reduction=numeric(), cat.tot.phene.loss=numeric(), cat.avg.phene.loss=numeric(), cat.med.phene.loss=numeric(), cat.max.phene.loss=numeric())
targ.24.meta.comps[1,]<-comp.meta.data(parent.meta = d.meta.data, sampled.meta = t.meta.data) #compare the meta data and try and quantify how much was lost compared to the parent data
row.names(targ.24.meta.comps)[1]<-"sample_1" #name it the same 
#now loop through the rest of the samples
for(n in 2:1000){#the number of samples we want to try 
  t.sample<-fix.sub.sample(target.df = sampling.data, fixed.names = c("Beauregard", "Tanzania", "Ruddy", "Porto Rico"), clust.colname="Cluster_24") #the sampling
  t.meta.data<-get.meta.data(t.sample) # extract meta data
  targ.24.samples[n,]<-t.sample$Code #save the sampled line codes
  samp.name<-paste0(c("sample", n), collapse="_") #generate unique sample name
  row.names(targ.24.samples)[n]<-samp.name #name the sampled data
  targ.24.meta.comps[n,]<-comp.meta.data(parent.meta = d.meta.data, sampled.meta = t.meta.data) #compare the meta data and try and quantify how much was lost compared to the parent data
  row.names(targ.24.meta.comps)[n]<-samp.name #name it the same 
}

#Multi-Criteria Decision Makeing
#let's see if we can find any sampled core sets that are in the top 21% for all criteria
thres.m.dev<-quantile(targ.24.meta.comps$quant.avg.mean.deviation, .2 , na.rm = TRUE)
thres.avg.spred<-quantile(targ.24.meta.comps$quant.avg.spread.reduction, .2 , na.rm = TRUE)
thres.max.spred<-quantile(targ.24.meta.comps$quant.max.spread.reduction, .2 , na.rm = TRUE)
thres.tot.phene.loss<-quantile(targ.24.meta.comps$cat.tot.phene.loss, .2 , na.rm = TRUE)
thres.avg.phene.loss<-quantile(targ.24.meta.comps$cat.avg.phene.loss, .2 , na.rm = TRUE)
thres.max.phene.loss<-quantile(targ.24.meta.comps$cat.max.phene.loss, .2 , na.rm = TRUE)

#only two core samples were in the top 20% for all 6 criteria
Top.samples.24 <- targ.24.meta.comps[(targ.24.meta.comps$quant.avg.mean.deviation<=thres.m.dev & targ.24.meta.comps$quant.avg.spread.reduction<=thres.avg.spred & targ.24.meta.comps$quant.max.spread.reduction<=thres.max.spred & targ.24.meta.comps$cat.tot.phene.loss<=thres.tot.phene.loss & targ.24.meta.comps$cat.avg.phene.loss<=thres.avg.phene.loss & targ.24.meta.comps$cat.max.phene.loss<=thres.max.phene.loss), ]

#the 24 line core sample #280 had the best (ie least loss) in 3 of the 6 selected metadata criteria 
names_selected_core_24<-targ.24.samples[280,]
ind.24<-sampling.data$Code %in% names_selected_core_24
selected_core_24.280<- sampling.data[ind.24,]
  
write.csv(selected_core_24.280, file = "selected_24_core_8-3-21.csv")

######48-line core selection#####

#Now we move onto the 48-line core set selection fixing all of those that were selected in the 24 line set
#names.fixed<-as.character(selected_core_24.794$`Common Name`)
PI.fixed<-as.character(selected_core_24.280$`GRIN Database PI Accession`)# because of the duplicate common names problem I had to convert this to PI both here and in the function
#run through the first sample to make sure it's doing what we want
t.sample<-fix.sub.sample(target.df = sampling.data, fixed.PIs = PI.fixed, clust.colname="Cluster_48") #the sampling, Note: this will throw a warning message when more than one fixed individual is designated "the condition has length > 1 and only the first element will be used" don't worry we only have to use the first element in that if statement, and all individuals designated will be fixed in the sample
t.meta.data<-get.meta.data(t.sample) # extracting meta data for this particular sampled core set
targ.48.samples<-data.frame(matrix(NA, nrow = 1, ncol = 48)) # generating the matrix to fill with selected core sets
targ.48.samples[1,]<-t.sample$Code #save the sampled line codes
row.names(targ.48.samples)[1]<-"sample_1" #name the sampled data
targ.48.meta.comps<-data.frame(quant.tot.mean.deviation=numeric(), quant.avg.mean.deviation=numeric(), quant.tot.sd.deviation=numeric(), quant.avg.sd.deviation=numeric(), quant.avg.spread.reduction=numeric(), quant.med.spread.reduction=numeric(), quant.max.spread.reduction=numeric(), cat.tot.phene.loss=numeric(), cat.avg.phene.loss=numeric(), cat.med.phene.loss=numeric(), cat.max.phene.loss=numeric())
targ.48.meta.comps[1,]<-comp.meta.data(parent.meta = d.meta.data, sampled.meta = t.meta.data) #compare the meta data and try and quantify how much was lost compared to the parent data
row.names(targ.48.meta.comps)[1]<-"sample_1" #name it the same 
#now loop through the rest of the samples
for(n in 2:1000){#the number of samples we want to try 
  t.sample<-fix.sub.sample(target.df = sampling.data, fixed.PIs = PI.fixed, clust.colname="Cluster_48") #the sampling
  t.meta.data<-get.meta.data(t.sample) # extract meta data
  targ.48.samples[n,]<-t.sample$Code #save the sampled line codes
  samp.name<-paste0(c("sample", n), collapse="_") #generate unique sample name
  row.names(targ.48.samples)[n]<-samp.name #name the sampled data
  targ.48.meta.comps[n,]<-comp.meta.data(parent.meta = d.meta.data, sampled.meta = t.meta.data) #compare the meta data and try and quantify how much was lost compared to the parent data
  row.names(targ.48.meta.comps)[n]<-samp.name #name it the same 
}

#Multi-Criteria Decision Makeing
#let's see if we can find any sampled core sets that are in the top 20% for all criteria
thres.m.dev<-quantile(targ.48.meta.comps$quant.avg.mean.deviation, .15 , na.rm = TRUE)
thres.avg.spred<-quantile(targ.48.meta.comps$quant.avg.spread.reduction, .15 , na.rm = TRUE)
thres.max.spred<-quantile(targ.48.meta.comps$quant.max.spread.reduction, .15 , na.rm = TRUE)
thres.tot.phene.loss<-quantile(targ.48.meta.comps$cat.tot.phene.loss, .15 , na.rm = TRUE)
thres.avg.phene.loss<-quantile(targ.48.meta.comps$cat.avg.phene.loss, .15 , na.rm = TRUE)
thres.max.phene.loss<-quantile(targ.48.meta.comps$cat.max.phene.loss, .15 , na.rm = TRUE)

#only 2 core samples were in the top 15% for all 6 criteria
Top.samples.48 <- targ.48.meta.comps[(targ.48.meta.comps$quant.avg.mean.deviation<=thres.m.dev & targ.48.meta.comps$quant.avg.spread.reduction<=thres.avg.spred & targ.48.meta.comps$quant.max.spread.reduction<=thres.max.spred & targ.48.meta.comps$cat.tot.phene.loss<=thres.tot.phene.loss & targ.48.meta.comps$cat.avg.phene.loss<=thres.avg.phene.loss & targ.48.meta.comps$cat.max.phene.loss<=thres.max.phene.loss), ]

#the 48-line core sample #369 had the best (ie least loss) in 6 of the 6 selected metadata criteria 
names_selected_core_48<-targ.48.samples[369,]
ind.48<-sampling.data$Code %in% names_selected_core_48
selected_core_48.369<- sampling.data[ind.48,]
#writing to CSV file
write.csv(selected_core_48.369, file = "selected_48_core_8-3-21.csv")

###### 96-line core selection #####

#Now we move onto the 96-line core set selection fixing all of those that were selected in the 48 line set
PI.fixed<-as.character(selected_core_48.369$`GRIN Database PI Accession`)
#run through the first sample to make sure it's doing what we want
t.sample<-fix.sub.sample(target.df = sampling.data, fixed.PIs = PI.fixed, clust.colname="Cluster_96") #the sampling, Note: this will throw a warning message when more than one fixed individual is designated "the condition has length > 1 and only the first element will be used" don't worry we only have to use the first element in that if statement, and all individuals designated will be fixed in the sample
t.meta.data<-get.meta.data(t.sample) # extracting meta data for this particular sampled core set
targ.96.samples<-data.frame(matrix(NA, nrow = 1, ncol = 96)) # generating the matrix to fill with selected core sets
targ.96.samples[1,]<-t.sample$Code #save the sampled line codes
row.names(targ.96.samples)[1]<-"sample_1" #name the sampled data
targ.96.meta.comps<-data.frame(quant.tot.mean.deviation=numeric(), quant.avg.mean.deviation=numeric(), quant.tot.sd.deviation=numeric(), quant.avg.sd.deviation=numeric(), quant.avg.spread.reduction=numeric(), quant.med.spread.reduction=numeric(), quant.max.spread.reduction=numeric(), cat.tot.phene.loss=numeric(), cat.avg.phene.loss=numeric(), cat.med.phene.loss=numeric(), cat.max.phene.loss=numeric())
targ.96.meta.comps[1,]<-comp.meta.data(parent.meta = d.meta.data, sampled.meta = t.meta.data) #compare the meta data and try and quantify how much was lost compared to the parent data
row.names(targ.96.meta.comps)[1]<-"sample_1" #name it the same 
#now loop through the rest of the samples
for(n in 2:1000){#the number of samples we want to try 
  t.sample<-fix.sub.sample(target.df = sampling.data, fixed.PIs = PI.fixed, clust.colname="Cluster_96") #the sampling
  t.meta.data<-get.meta.data(t.sample) # extract meta data
  targ.96.samples[n,]<-t.sample$Code #save the sampled line codes
  samp.name<-paste0(c("sample", n), collapse="_") #generate unique sample name
  row.names(targ.96.samples)[n]<-samp.name #name the sampled data
  targ.96.meta.comps[n,]<-comp.meta.data(parent.meta = d.meta.data, sampled.meta = t.meta.data) #compare the meta data and try and quantify how much was lost compared to the parent data
  row.names(targ.96.meta.comps)[n]<-samp.name #name it the same 
}

#Multi-Criteria Decision Makeing
#let's see if we can find any sampled core sets that are in the top 20% for all criteria
thres.m.dev<-quantile(targ.96.meta.comps$quant.avg.mean.deviation, .2 , na.rm = TRUE)
thres.avg.spred<-quantile(targ.96.meta.comps$quant.avg.spread.reduction, .2 , na.rm = TRUE)
thres.max.spred<-quantile(targ.96.meta.comps$quant.max.spread.reduction, .2 , na.rm = TRUE)
thres.tot.phene.loss<-quantile(targ.96.meta.comps$cat.tot.phene.loss, .2 , na.rm = TRUE)
thres.avg.phene.loss<-quantile(targ.96.meta.comps$cat.avg.phene.loss, .2 , na.rm = TRUE)
thres.max.phene.loss<-quantile(targ.96.meta.comps$cat.max.phene.loss, .2 , na.rm = TRUE)

#only 3 core samples were in the top 20% for all 6 criteria
Top.samples.96 <- targ.96.meta.comps[(targ.96.meta.comps$quant.avg.mean.deviation<=thres.m.dev & targ.96.meta.comps$quant.avg.spread.reduction<=thres.avg.spred & targ.96.meta.comps$quant.max.spread.reduction<=thres.max.spred & targ.96.meta.comps$cat.tot.phene.loss<=thres.tot.phene.loss & targ.96.meta.comps$cat.avg.phene.loss<=thres.avg.phene.loss & targ.96.meta.comps$cat.max.phene.loss<=thres.max.phene.loss), ]

#the 96-line core sample #285 had the best (ie least loss) in 3 of the 6 selected metadata criteria, and second least loss in 2 other meta criteria
names_selected_core_96<-targ.96.samples[285,]
ind.96<-sampling.data$Code %in% names_selected_core_96
selected_core_96.285<- sampling.data[ind.96,]
#writing to CSV file
write.csv(selected_core_96.285, file = "selected_96_core_8-3-21.csv")


###### 384-line core selection #####

#Now we move onto the 384-line core set selection fixing all of those that were selected in the 96 line set
PI.fixed<-as.character(selected_core_96.285$`GRIN Database PI Accession`)
#run through the first sample to make sure it's doing what we want
t.sample<-fix.sub.sample(target.df = sampling.data, fixed.PIs = PI.fixed, clust.colname="Cluster_384") #the sampling, Note: this will throw a warning message when more than one fixed individual is designated "the condition has length > 1 and only the first element will be used" don't worry we only have to use the first element in that if statement, and all individuals designated will be fixed in the sample
t.meta.data<-get.meta.data(t.sample) # extracting meta data for this particular sampled core set
targ.384.samples<-data.frame(matrix(NA, nrow = 1, ncol = 384)) # generating the matrix to fill with selected core sets
targ.384.samples[1,]<-t.sample$Code #save the sampled line codes
row.names(targ.384.samples)[1]<-"sample_1" #name the sampled data
targ.384.meta.comps<-data.frame(quant.tot.mean.deviation=numeric(), quant.avg.mean.deviation=numeric(), quant.tot.sd.deviation=numeric(), quant.avg.sd.deviation=numeric(), quant.avg.spread.reduction=numeric(), quant.med.spread.reduction=numeric(), quant.max.spread.reduction=numeric(), cat.tot.phene.loss=numeric(), cat.avg.phene.loss=numeric(), cat.med.phene.loss=numeric(), cat.max.phene.loss=numeric())
targ.384.meta.comps[1,]<-comp.meta.data(parent.meta = d.meta.data, sampled.meta = t.meta.data) #compare the meta data and try and quantify how much was lost compared to the parent data
row.names(targ.384.meta.comps)[1]<-"sample_1" #name it the same 
#now loop through the rest of the samples
for(n in 2:1000){#the number of samples we want to try 
  t.sample<-fix.sub.sample(target.df = sampling.data, fixed.PIs = PI.fixed, clust.colname="Cluster_384") #the sampling
  t.meta.data<-get.meta.data(t.sample) # extract meta data
  targ.384.samples[n,]<-t.sample$Code #save the sampled line codes
  samp.name<-paste0(c("sample", n), collapse="_") #generate unique sample name
  row.names(targ.384.samples)[n]<-samp.name #name the sampled data
  targ.384.meta.comps[n,]<-comp.meta.data(parent.meta = d.meta.data, sampled.meta = t.meta.data) #compare the meta data and try and quantify how much was lost compared to the parent data
  row.names(targ.384.meta.comps)[n]<-samp.name #name it the same 
}

#Multi-Criteria Decision Makeing
#let's see if we can find any sampled core sets that are in the top 10% for all criteria
thres.m.dev<-quantile(targ.384.meta.comps$quant.avg.mean.deviation, .1 , na.rm = TRUE)
thres.avg.spred<-quantile(targ.384.meta.comps$quant.avg.spread.reduction, .1 , na.rm = TRUE)
thres.max.spred<-quantile(targ.384.meta.comps$quant.max.spread.reduction, .1 , na.rm = TRUE)
thres.tot.phene.loss<-quantile(targ.384.meta.comps$cat.tot.phene.loss, .1 , na.rm = TRUE)
thres.avg.phene.loss<-quantile(targ.384.meta.comps$cat.avg.phene.loss, .1 , na.rm = TRUE)
thres.max.phene.loss<-quantile(targ.384.meta.comps$cat.max.phene.loss, .1 , na.rm = TRUE)

#only 2 core sample were in the top 10% for all 6 criteria
Top.samples.384 <- targ.384.meta.comps[(targ.384.meta.comps$quant.avg.mean.deviation<=thres.m.dev & targ.384.meta.comps$quant.avg.spread.reduction<=thres.avg.spred & targ.384.meta.comps$quant.max.spread.reduction<=thres.max.spred & targ.384.meta.comps$cat.tot.phene.loss<=thres.tot.phene.loss & targ.384.meta.comps$cat.avg.phene.loss<=thres.avg.phene.loss & targ.384.meta.comps$cat.max.phene.loss<=thres.max.phene.loss), ]

#the 384-line core sample #548 had the best (ie least loss) in 5 of the 6 selected metadata criteria 
names_selected_core_384<-targ.384.samples[548,]
ind.384<-sampling.data$Code %in% names_selected_core_384
selected_core_384.548<- sampling.data[ind.384,]
#writing to CSV file
write.csv(selected_core_384.548, file = "selected_384_core_8-3-21.csv")

#save all the selected cores and metadata comparison datasets
save(targ.24.samples, targ.48.samples, targ.96.samples, targ.384.samples, file = "all_core_sample_selections_8-3-21.RData")
save(targ.24.meta.comps, targ.48.meta.comps, targ.96.meta.comps, targ.384.meta.comps, file = "all_meta_data_comp_8-3-21.RData")

#confirming that they are all nested
sum(selected_core_24.280$Code %in% selected_core_48.369$Code)
sum(selected_core_48.369$Code %in% selected_core_96.285$Code)
sum(selected_core_96.285$Code %in% selected_core_384.548$Code)

#confirming the number of unique geneic clusters represented in each core set
length(unique(selected_core_24.280$Cluster_24))
length(unique(selected_core_48.369$Cluster_48))
length(unique(selected_core_96.285$Cluster_96))
length(unique(selected_core_384.548$Cluster_384)) 
